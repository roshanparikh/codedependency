{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a913996d",
   "metadata": {},
   "source": [
    "# Import modules, data, and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8dc3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "import os, sys\n",
    "from os.path import join, dirname, abspath, normpath\n",
    "from pickle import load as pkl_load, dump as pkl_dump\n",
    "import shap\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "import sklearn\n",
    "from scipy.stats import chi2_contingency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9433db2a",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccf0260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_shap_on_model(model, X_preprocessed: pd.DataFrame, background_samples: Optional[int] = 100, seed: int = 42):\n",
    "    \"\"\"\n",
    "    Run SHAP on a fitted model and preprocessed dataset.\n",
    "\n",
    "    Args:\n",
    "        model: fitted model object (sklearn/XGBoost/...) that supports prediction.\n",
    "        X_preprocessed: preprocessed features as DataFrame or array used for SHAP evaluation.\n",
    "        background_samples: number of background rows to use for the explainer (int) or None to use all.\n",
    "        seed: RNG seed for reproducible sampling.\n",
    "\n",
    "    Returns:\n",
    "        shap_values: object returned by shap.Explainer(...)(X_preprocessed). Contains .values, .base_values, .data, .feature_names when available.\n",
    "    \"\"\"\n",
    "    # Convert to DataFrame if needed (so feature names are preserved)\n",
    "    if not isinstance(X_preprocessed, pd.DataFrame):\n",
    "        try:\n",
    "            X_preprocessed = pd.DataFrame(X_preprocessed)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Choose a background dataset for the explainer (smaller is faster)\n",
    "    if background_samples is None or (isinstance(background_samples, int) and background_samples >= len(X_preprocessed)):\n",
    "        background = X_preprocessed\n",
    "    else:\n",
    "        background = X_preprocessed.sample(min(background_samples, len(X_preprocessed)), random_state=seed)\n",
    "\n",
    "    # Use shap.Explainer which auto-selects the best explainer for the model type\n",
    "    if type(model) in [sklearn.pipeline.Pipeline]:\n",
    "        if hasattr(model[-1], 'predict_proba'):\n",
    "            explainer = shap.Explainer(lambda X: model.predict_proba(X)[:,1], background)\n",
    "        elif hasattr(model[-1], 'decision_function'):\n",
    "            explainer = shap.Explainer(lambda X: model.decision_function(X), background)\n",
    "        elif hasattr(model[-1], 'predict'):\n",
    "            explainer = shap.Explainer(lambda X: model.predict(X.values), background)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Model pipeline's final step does not have predict_proba, decision_function, or predict method.\")\n",
    "    else:\n",
    "        if hasattr(model, 'decision_function'):\n",
    "            print(\"Using decision_function for SHAP explainer.\")\n",
    "            explainer = shap.Explainer(lambda X: model.decision_function(X), background)\n",
    "        else:\n",
    "            explainer = shap.Explainer(model, background)\n",
    "\n",
    "    # Compute SHAP values for the full preprocessed set\n",
    "    shap_values = explainer(X_preprocessed)\n",
    "\n",
    "    return shap_values\n",
    "\n",
    "\n",
    "def save_shap_results(shap_values, path: str):\n",
    "    \"\"\"\n",
    "    Save shap results (arrays and metadata) to disk using pickle.\n",
    "\n",
    "    This stores a small dict with arrays so the heavy shap.Explanation object isn't required to be re-created.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(path) or '.', exist_ok=True)\n",
    "\n",
    "    payload = {\n",
    "        'values': getattr(shap_values, 'values', None),\n",
    "        'base_values': getattr(shap_values, 'base_values', None),\n",
    "        'data': getattr(shap_values, 'data', None),\n",
    "        'feature_names': getattr(shap_values, 'feature_names', None),\n",
    "        # also save the explanation's output names if ppayloadent\n",
    "        'output_names': getattr(shap_values, 'output_names', None),\n",
    "        'explanation': shap_values\n",
    "    }\n",
    "\n",
    "    with open(path, 'wb') as f:\n",
    "        pkl_dump(payload, f)\n",
    "\n",
    "    return path\n",
    "\n",
    "\n",
    "def plot_shap_summary_plotly(shap_values, max_features: int = 20, title: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    Create a Plotly horizontal bar chart showing mean(|SHAP value|) per feature.\n",
    "\n",
    "    Args:\n",
    "        shap_values: the result object from shap.Explainer(...)(X)\n",
    "        max_features: max number of features to show (top by mean abs SHAP)\n",
    "        title: optional chart title\n",
    "\n",
    "    Returns:\n",
    "        fig: plotly.graph_objects.Figure\n",
    "    \"\"\"\n",
    "    # Extract numeric array; handle multiclass shape\n",
    "    vals = getattr(shap_values, 'values', None)\n",
    "    if vals is None:\n",
    "        raise ValueError(\"shap_values has no .values attribute\")\n",
    "\n",
    "    # If multiclass, shap returns shape (n_samples, n_features) for single-output or (n_samples, n_classes, n_features)\n",
    "    if vals.ndim == 3:\n",
    "        # reduce across classes by taking the mean absolute across classes\n",
    "        vals_reduced = np.mean(np.abs(vals), axis=1)  # (n_samples, n_features)\n",
    "    elif vals.ndim == 2:\n",
    "        vals_reduced = vals\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected shap values shape: {vals.shape}\")\n",
    "\n",
    "    mean_abs = np.mean(np.abs(vals_reduced), axis=0)\n",
    "\n",
    "    # Feature names\n",
    "    feature_names = getattr(shap_values, 'feature_names', None)\n",
    "    if feature_names is None:\n",
    "        # try to get from data attribute column names\n",
    "        data = getattr(shap_values, 'data', None)\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            feature_names = data.columns.tolist()\n",
    "        elif isinstance(data, np.ndarray):\n",
    "            feature_names = [f'f{i}' for i in range(data.shape[1])]\n",
    "        else:\n",
    "            feature_names = [f'f{i}' for i in range(len(mean_abs))]\n",
    "\n",
    "    df = pd.DataFrame({'feature': feature_names, 'mean_abs_shap': mean_abs})\n",
    "    df = df.sort_values('mean_abs_shap', ascending=True).tail(max_features)\n",
    "\n",
    "    fig = px.bar(df, x='mean_abs_shap', y='feature', orientation='h', title=title,\n",
    "                 labels={'mean_abs_shap': 'mean(|SHAP value|)', 'feature': 'Feature'})\n",
    "    fig.update_layout(margin=dict(l=200, r=20, t=60, b=40), height=50 + 30 * len(df))\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def save_plotly_figure(fig, path: str):\n",
    "    \"\"\"\n",
    "    Save a Plotly figure to disk. If the path extension is .html, writes interactive HTML. Otherwise\n",
    "    attempts to save a static image (requires kaleido) and falls back to HTML if that fails.\n",
    "\n",
    "    Returns the path actually written.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(path) or '.', exist_ok=True)\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "\n",
    "    if ext == '.html':\n",
    "        fig.write_html(path)\n",
    "        return path\n",
    "\n",
    "    # try saving as image (png, svg, jpeg, pdf) if kaleido is installed\n",
    "    try:\n",
    "        fig.write_image(path)\n",
    "        return path\n",
    "    except Exception as e:\n",
    "        # fallback to html (same filename with .html)\n",
    "        fallback = path + '.html'\n",
    "        fig.write_html(fallback)\n",
    "        print(f\"Could not write image directly ({e}). Saved interactive HTML to {fallback} instead.\")\n",
    "        return fallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a37c59",
   "metadata": {},
   "source": [
    "# Run SHAP analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559cdae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"output/shap\"\n",
    "experiment_names = ['model_results_immune_90', \n",
    "                    'model_results_immune_30', \n",
    "                    'model_results_all_90',\n",
    "                    'model_results_all_30',\n",
    "                    'model_results_labs_90',\n",
    "                    'model_results_labs_30'\n",
    "                    ]\n",
    "model_names = [\n",
    "    'XGB', \n",
    "    'KNN', \n",
    "    'SVC Linear', \n",
    "    'SVC RBF', \n",
    "    'SVC Poly'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dfccc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd77d8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much data should we use when running shap? arger sample = Longer runtime.\n",
    "# None -> use all the available data\n",
    "SAMPLE_SIZE = None\n",
    "\n",
    "for experiment_name in experiment_names:\n",
    "    with open(f\"{experiment_name}.pkl\", 'rb') as f:\n",
    "        experiment = pkl_load(f)\n",
    "    #experiment = globals().get(experiment_name)\n",
    "\n",
    "    for trial_name in model_names:\n",
    "        RESULTS_PATH = join(OUTPUT_DIR, experiment_name, trial_name)\n",
    "        \n",
    "        trial = experiment[trial_name]\n",
    "        X_pre = trial['X_test_preprocessed'] # Column renaming necessary\n",
    "        X_raw = trial['X_test_raw']\n",
    "        model = trial['best_model']\n",
    "\n",
    "        print(f\"Running SHAP on {experiment_name}:{trial_name}...\")\n",
    "        shap_vals = run_shap_on_model(model[1], X_pre, background_samples=SAMPLE_SIZE)\n",
    "        shap_vals.feature_names = X_pre.columns.tolist()\n",
    "\n",
    "        # Save SHAP results and plots\n",
    "        shap_out_path = os.path.join(RESULTS_PATH, f\"shap_{experiment_name}_{trial_name}.pkl\")\n",
    "        save_shap_results(shap_vals, shap_out_path)\n",
    "        print('Saved SHAP arrays to', shap_out_path)\n",
    "\n",
    "        fig = plot_shap_summary_plotly(shap_vals, max_features=25, title=f\"SHAP mean(|value|) - Immune {trial_name}\")\n",
    "        chart_out = save_plotly_figure(fig, os.path.join(RESULTS_PATH, f\"shap_summary_{experiment_name}_{trial_name}.html\"))\n",
    "        chart_out = save_plotly_figure(fig, os.path.join(RESULTS_PATH, f\"shap_summary_{experiment_name}_{trial_name}.jpeg\"))\n",
    "        print('Saved SHAP chart to', chart_out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd463cb4",
   "metadata": {},
   "source": [
    "## Create a better shap plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8130b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pkl_load(f)\n",
    "\n",
    "def _as_2d(values, class_index=None):\n",
    "    \"\"\"\n",
    "    Normalize SHAP `values` to shape (n_samples, n_features).\n",
    "    - If it's a list (multi-output), pick class_index (default: 1 if it exists else 0).\n",
    "    - If it's 1D, add a leading sample axis.\n",
    "    \"\"\"\n",
    "    if isinstance(values, list):\n",
    "        if class_index is None:\n",
    "            class_index = 1 if len(values) > 1 else 0\n",
    "        values = values[class_index]\n",
    "    values = np.asarray(values)\n",
    "    if values.ndim == 1:\n",
    "        values = values[np.newaxis, :]\n",
    "    return values\n",
    "\n",
    "def plot_top_pos_neg_from_payload(\n",
    "    dir: str,\n",
    "    pickle_name: str,\n",
    "    row_index: int = 0,\n",
    "    top_k: int = 5,\n",
    "    class_index: int | None = None,\n",
    "    title: str | None = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Load SHAP payload (values/base_values/data/feature_names/output_names) and\n",
    "    plot the top-K positive and negative SHAP contributions for a single row.\n",
    "    Returns a Plotly Figure.\n",
    "    \"\"\"\n",
    "    payload = load_pickle(join(dir,pickle_name))\n",
    "\n",
    "    values = payload.get(\"values\", None)\n",
    "    if values is None:\n",
    "        raise ValueError(\"payload['values'] is missing; please ensure you saved shap_values.values.\")\n",
    "\n",
    "    # Normalize to (n_samples, n_features)\n",
    "    S = _as_2d(values, class_index=class_index)\n",
    "\n",
    "    # Feature names\n",
    "    feature_names = payload.get(\"feature_names\", None)\n",
    "    if feature_names is None:\n",
    "        feature_names = [f\"f{i}\" for i in range(S.shape[1])]\n",
    "    feature_names = np.array(feature_names)\n",
    "\n",
    "    # Optional raw feature values for hover\n",
    "    data = payload.get(\"data\", None)\n",
    "    if data is not None:\n",
    "        data = np.asarray(data)\n",
    "        if data.ndim == 1:\n",
    "            data = data[np.newaxis, :]\n",
    "\n",
    "    # Select the target row\n",
    "    if row_index < 0 or row_index >= S.shape[0]:\n",
    "        raise IndexError(f\"row_index {row_index} out of range (0..{S.shape[0]-1}).\")\n",
    "    s = S[row_index]  # shap vector (n_features,)\n",
    "\n",
    "    # Indices for top positive & negative contributions\n",
    "    k = min(top_k, s.size)\n",
    "    pos_idx = np.argpartition(-s, kth=k-1)[:k]\n",
    "    neg_idx = np.argpartition( s, kth=k-1)[:k]\n",
    "    sel_idx = np.unique(np.concatenate([pos_idx, neg_idx]))\n",
    "\n",
    "    # Build plotting frame\n",
    "    dfp = pd.DataFrame({\n",
    "        \"feature\": feature_names[sel_idx],\n",
    "        \"shap\": s[sel_idx],\n",
    "        \"direction\": np.where(s[sel_idx] >= 0, \"positive\", \"negative\"),\n",
    "    })\n",
    "    if data is not None and data.shape[1] == s.size:\n",
    "        dfp[\"value\"] = data[row_index, sel_idx]\n",
    "    else:\n",
    "        dfp[\"value\"] = np.nan\n",
    "\n",
    "    # Sort so negatives appear together (left) then positives (right)\n",
    "    dfp = dfp.sort_values(\"shap\")\n",
    "\n",
    "    # Title\n",
    "    if title is None:\n",
    "        title = f\"Top Â±{top_k} SHAP contributions (row {row_index})\"\n",
    "\n",
    "    # Plotly horizontal bar\n",
    "    fig = px.bar(\n",
    "        dfp, x=\"shap\", y=\"feature\", orientation=\"h\",\n",
    "        color=\"direction\",\n",
    "        color_discrete_map={\"positive\": \"#2ca02c\", \"negative\": \"#d62728\"},\n",
    "        hover_data={\"value\": True, \"shap\":\":.4f\", \"direction\": False}\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title=\"SHAP value (impact on model output)\",\n",
    "        yaxis_title=\"\",\n",
    "        showlegend=False,\n",
    "        bargap=0.25,\n",
    "    )\n",
    "    # Add text labels for quick read\n",
    "    fig.update_traces(text=dfp[\"shap\"].map(lambda v: f\"{v:+.3f}\"), textposition=\"outside\", cliponaxis=False)\n",
    "\n",
    "    # Optional save\n",
    "    save_html = join(dir, f\"shap_top_pos_neg_row{row_index}.html\")\n",
    "    save_png = join(dir, f\"shap_top_pos_neg_row{row_index}.png\")\n",
    "    if save_html:\n",
    "        os.makedirs(os.path.dirname(save_html) or \".\", exist_ok=True)\n",
    "        fig.write_html(save_html)\n",
    "    if save_png:\n",
    "        os.makedirs(os.path.dirname(save_png) or \".\", exist_ok=True)\n",
    "        # Requires kaleido installed: pip install -U kaleido\n",
    "        fig.write_image(save_png, scale=2)\n",
    "\n",
    "    return fig\n",
    "\n",
    "def save_fig(fig, dir, name):\n",
    "    fig.write_html(join(dir, f\"{name}.html\"))\n",
    "    fig.write_image(join(dir, f\"{name}.jpeg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9633cb87",
   "metadata": {},
   "source": [
    "## Run Plot Generation for the best observed models\n",
    "\n",
    "1. (f1): All features + SVC RBF + 30 days\n",
    "\n",
    "2. (recall): Labs + SVC RBF + 30 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6942f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_1 = 'output/shap/model_results_all_30/SVC RBF/'\n",
    "file_1 = 'output/shap/model_results_all_30/SVC RBF/shap_model_results_all_30_SVC RBF.pkl'\n",
    "dir_2 = 'output/shap/model_results_labs_30/SVC RBF/'\n",
    "file_2 = 'output/shap/model_results_labs_30/SVC RBF/shap_model_results_labs_30_SVC RBF.pkl'\n",
    "\n",
    "file = file_1\n",
    "dir = dir_1\n",
    "\n",
    "payload = load_pickle(file)\n",
    "if 'explanation' in payload:\n",
    "    explainer = payload['explanation']\n",
    "else:\n",
    "    explainer = shap.Explanation(values=payload['values'], \n",
    "                           base_values=payload['base_values'], \n",
    "                           data=payload['data'], feature_names=payload['feature_names'], output_names=payload['output_names'])\n",
    "# Eliminate extraneous parts of feature names for readability\n",
    "colnames = [s.split(\"__\")[1].replace('/','-') for s in explainer.feature_names]\n",
    "explainer.feature_names = colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e69976",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "values = pd.DataFrame(explainer.values, columns=colnames)\n",
    "# 1. Bar plot of mean of abs of shap vals\n",
    "fig = plt.figure()\n",
    "shap.plots.bar(explainer, show=False)\n",
    "fig.savefig(join(dir, 'shap_abs_bar_plot.png'), dpi=300, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "# 2. Beeswarm plot\n",
    "fig = plt.figure()\n",
    "shap.plots.beeswarm(explainer, show=False)\n",
    "fig.savefig(join(dir, 'shap_beeswarm_plot.png'), dpi=300, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "# Top and bottom bar plot\n",
    "top_5 = values.mean().sort_values(ascending=False).head(5)\n",
    "bottom_5 = values.mean().sort_values(ascending=False).tail(5)\n",
    "top_bottom_5 = pd.concat([top_5, bottom_5])\n",
    "top_bottom_5 = pd.DataFrame({\n",
    "    \"feature_name\": top_bottom_5.index,\n",
    "    \"shap_value\": top_bottom_5.values,\n",
    "    \"color\": ['positive' if x > 0 else 'negative' for x in top_bottom_5]\n",
    "                             })\n",
    "fig = px.bar(data_frame=top_bottom_5, y=\"feature_name\", x=\"shap_value\", orientation='h', color='color', title='Top and Bottom 5 Features by Mean(SHAP value)')\n",
    "save_fig(fig, dir, 'shap_top_bottom_5_features')\n",
    "\n",
    "# 4. Scatter plots\n",
    "for feature in top_bottom_5['feature_name']:\n",
    "    fig = plt.figure()\n",
    "    shap.plots.scatter(explainer[:, feature], title=f'SHAP Scatter Plot for {feature}', show=False)\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(join(dir, f\"shap_scatter_{feature.replace('/','-')}.png\"), dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c9cfcf",
   "metadata": {},
   "source": [
    "## Calculate Correlation between a feature of interest and 30-day mortality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e61a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"30_day_mort.csv\")\n",
    "last_careunit = pd.get_dummies(df[\"last_careunit\"]).astype(int)\n",
    "last_careunit[\"mortality\"] = df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00fed23",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "for col in last_careunit.columns:\n",
    "    pct = (last_careunit[col] & last_careunit[\"mortality\"]).sum() / last_careunit[col].sum()\n",
    "    d[col] = pct*100\n",
    "corrs = last_careunit.corr()[\"mortality\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0e6f29",
   "metadata": {},
   "source": [
    "Get correlation coefficient and Odds Ratio for CVICU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c58130",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = \"Cardiac Vascular Intensive Care Unit (CVICU)\"\n",
    "table = pd.crosstab(last_careunit[feature], last_careunit[\"mortality\"])\n",
    "chi2, p_value, dof, expected = chi2_contingency(table)\n",
    "print(f\"Chi-Squared Statistic: {chi2:.4f}\")\n",
    "print(f\"**P-Value:** {p_value:.4f}\")\n",
    "# Get phi coefficient for correlation\n",
    "n = table.sum().sum()  # Total number of observations\n",
    "phi = np.sqrt(chi2 / n)\n",
    "print(f\"Phi Coefficient: {phi:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e184553",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = table.loc[1, 1]  # Admitted, Died\n",
    "b = table.loc[0, 1]  # Not Admitted, Died\n",
    "c = table.loc[1, 0]  # Admitted, Lived\n",
    "d = table.loc[0, 0]  # Not Admitted, Lived\n",
    "\n",
    "# Calculate Odds Ratio: (a/c) / (b/d)  OR  (a*d) / (b*c)\n",
    "odds_ratio = (a * d) / (b * c)\n",
    "\n",
    "print(f\"**Odds Ratio (OR):** {odds_ratio:.4f}\")\n",
    "print(f\"Interpretation: The odds of 30-day mortality are {odds_ratio:.2f} times higher \"\n",
    "      \"for patients admitted to the unit compared to those not admitted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4c44f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_or = np.log(odds_ratio)\n",
    "\n",
    "# 2. Calculate the Standard Error (SE) of the log-OR\n",
    "se_log_or = np.sqrt( (1/a) + (1/b) + (1/c) + (1/d) )\n",
    "\n",
    "# 3. Get the 95% CI on the log scale\n",
    "z_score = 1.96  # for 95% CI\n",
    "log_ci_lower = log_or - z_score * se_log_or\n",
    "log_ci_upper = log_or + z_score * se_log_or\n",
    "\n",
    "# 4. Convert the CIs back to the normal scale\n",
    "ci_lower = np.exp(log_ci_lower)\n",
    "ci_upper = np.exp(log_ci_upper)\n",
    "\n",
    "print(f\"**Odds Ratio (OR):** {odds_ratio:.2f}\")\n",
    "print(f\"**95% Confidence Interval (CI):** [{ci_lower:.2f} - {ci_upper:.2f}]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
