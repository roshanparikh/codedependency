Sender: LSF System <lsfadmin@lg07c06>
Subject: Job 211049024: <#BSUB -P acc_bedsore_images;#BSUB -q gpu;#BSUB -gpu num=1;#BSUB -R a100;#BSUB -R "span[hosts=1]";#BSUB -n 4;#BSUB -R "rusage[mem=16000]";#BSUB -W 12:00;#BSUB -o datathon_train_labs.out;#BSUB -e datathon_train_labs.err; set -euo pipefail; # Modules;ml purge;ml cuda/11.8.0;ml anaconda3/2024.06; # Conda activate (non-interactive safe);eval "$(conda shell.bash hook)";conda activate pressureulcerstudy; # Ensure CUDA runtime is visible for TF;export LD_LIBRARY_PATH="${CUDA_HOME}/lib64:${LD_LIBRARY_PATH:-}"; # Thread/env knobs;export OMP_NUM_THREADS=1;export MKL_NUM_THREADS=1;export NUMEXPR_NUM_THREADS=1;export TF_NUM_INTEROP_THREADS=1;export TF_NUM_INTRAOP_THREADS=1;export TF_CPP_MIN_LOG_LEVEL=1;export TF_FORCE_GPU_ALLOW_GROWTH=true;export PYTHONUNBUFFERED=1; # Train;python -u train_and_eval_labs.py> in cluster <chimera> Done

Job <#BSUB -P acc_bedsore_images;#BSUB -q gpu;#BSUB -gpu num=1;#BSUB -R a100;#BSUB -R "span[hosts=1]";#BSUB -n 4;#BSUB -R "rusage[mem=16000]";#BSUB -W 12:00;#BSUB -o datathon_train_labs.out;#BSUB -e datathon_train_labs.err; set -euo pipefail; # Modules;ml purge;ml cuda/11.8.0;ml anaconda3/2024.06; # Conda activate (non-interactive safe);eval "$(conda shell.bash hook)";conda activate pressureulcerstudy; # Ensure CUDA runtime is visible for TF;export LD_LIBRARY_PATH="${CUDA_HOME}/lib64:${LD_LIBRARY_PATH:-}"; # Thread/env knobs;export OMP_NUM_THREADS=1;export MKL_NUM_THREADS=1;export NUMEXPR_NUM_THREADS=1;export TF_NUM_INTEROP_THREADS=1;export TF_NUM_INTRAOP_THREADS=1;export TF_CPP_MIN_LOG_LEVEL=1;export TF_FORCE_GPU_ALLOW_GROWTH=true;export PYTHONUNBUFFERED=1; # Train;python -u train_and_eval_labs.py> was submitted from host <li04e01> by user <parikr12> in cluster <chimera> at Sat Nov  8 15:01:50 2025
Job was executed on host(s) <4*lg07c06>, in queue <gpu>, as user <parikr12> in cluster <chimera> at Sat Nov  8 15:01:54 2025
</hpc/users/parikr12> was used as the home directory.
</sc/arion/projects/bedsore_images/Datathon/codedependency> was used as the working directory.
Started at Sat Nov  8 15:01:54 2025
Terminated at Sat Nov  8 15:16:14 2025
Results reported at Sat Nov  8 15:16:14 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -P acc_bedsore_images
#BSUB -q gpu
#BSUB -gpu num=1
#BSUB -R a100
#BSUB -R "span[hosts=1]"
#BSUB -n 4
#BSUB -R "rusage[mem=16000]"
#BSUB -W 12:00
#BSUB -o datathon_train_labs.out
#BSUB -e datathon_train_labs.err

set -euo pipefail

# Modules
ml purge
ml cuda/11.8.0
ml anaconda3/2024.06

# Conda activate (non-interactive safe)
eval "$(conda shell.bash hook)"
conda activate pressureulcerstudy

# Ensure CUDA runtime is visible for TF
export LD_LIBRARY_PATH="${CUDA_HOME}/lib64:${LD_LIBRARY_PATH:-}"

# Thread/env knobs
export OMP_NUM_THREADS=1
export MKL_NUM_THREADS=1
export NUMEXPR_NUM_THREADS=1
export TF_NUM_INTEROP_THREADS=1
export TF_NUM_INTRAOP_THREADS=1
export TF_CPP_MIN_LOG_LEVEL=1
export TF_FORCE_GPU_ALLOW_GROWTH=true
export PYTHONUNBUFFERED=1

# Train
python -u train_and_eval_labs.py
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   2368.00 sec.
    Max Memory :                                 705 MB
    Average Memory :                             675.27 MB
    Total Requested Memory :                     64000.00 MB
    Delta Memory :                               63295.00 MB
    Max Swap :                                   -
    Max Processes :                              10
    Max Threads :                                23
    Run time :                                   860 sec.
    Turnaround time :                            864 sec.

The output (if any) follows:


>>>Running XGBClassifier(base_score=None, booster=None, callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device=None, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=0.03, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=1000,
              n_jobs=None, num_parallel_tree=None, ...)
Fitting 5 folds for each of 100 candidates, totalling 500 fits
Grid fit complete
Predictions made
Making results
Results for XGB:
Accuracy: 0.7892
Accuracy improvement over baseline: -7.53%
F1 Score: 0.4459
F1 improvement over baseline: 44594594594594.59%
Recall: 0.5789473684210527
Best parameters: {'model__colsample_bytree': 0.25, 'model__max_depth': 3, 'model__scale_pos_weight': 15}

>>>Running KNeighborsClassifier()
Fitting 5 folds for each of 44 candidates, totalling 220 fits
Grid fit complete
Predictions made
Making results
Results for KNN:
Accuracy: 0.8046
Accuracy improvement over baseline: -5.72%
F1 Score: 0.2692
F1 improvement over baseline: 26923076923076.92%
Recall: 0.24561403508771928
Best parameters: {'model__n_neighbors': 1, 'model__p': 1, 'model__weights': 'uniform'}

>>>Running SVC(kernel='linear', random_state=42)
Fitting 5 folds for each of 18 candidates, totalling 90 fits
Grid fit complete
Predictions made
Making results
Results for SVC Linear:
Accuracy: 0.7275
Accuracy improvement over baseline: -14.76%
F1 Score: 0.4362
F1 improvement over baseline: 43617021276595.74%
Recall: 0.7192982456140351
Best parameters: {'model__C': 1.0, 'model__class_weight': 'balanced'}

>>>Running SVC(random_state=42)
Fitting 5 folds for each of 18 candidates, totalling 90 fits
Grid fit complete
Predictions made
Making results
Results for SVC RBF:
Accuracy: 0.7481
Accuracy improvement over baseline: -12.35%
F1 Score: 0.4368
F1 improvement over baseline: 43678160919540.23%
Recall: 0.6666666666666666
Best parameters: {'model__C': 1.0, 'model__class_weight': 'balanced'}

>>>Running SVC(kernel='poly', random_state=42)
Fitting 5 folds for each of 640 candidates, totalling 3200 fits
Grid fit complete
Predictions made
Making results
Results for SVC Poly:
Accuracy: 0.7558
Accuracy improvement over baseline: -11.45%
F1 Score: 0.4633
F1 improvement over baseline: 46327683615819.21%
Recall: 0.7192982456140351
Best parameters: {'model__C': 0.01, 'model__class_weight': 'balanced', 'model__coef0': 1.0, 'model__degree': 3, 'model__gamma': 'scale'}


PS:

Read file <datathon_train_labs.err> for stderr output of this job.

Sender: LSF System <lsfadmin@lg07c07>
Subject: Job 211051347: <#BSUB -P acc_bedsore_images;#BSUB -q gpu;#BSUB -gpu num=1;#BSUB -R a100;#BSUB -R "span[hosts=1]";#BSUB -n 4;#BSUB -R "rusage[mem=16000]";#BSUB -W 12:00;#BSUB -o datathon_train_labs.out;#BSUB -e datathon_train_labs.err; set -euo pipefail; # Modules;ml purge;ml cuda/11.8.0;ml anaconda3/2024.06; # Conda activate (non-interactive safe);eval "$(conda shell.bash hook)";conda activate pressureulcerstudy; # Ensure CUDA runtime is visible for TF;export LD_LIBRARY_PATH="${CUDA_HOME}/lib64:${LD_LIBRARY_PATH:-}"; # Thread/env knobs;export OMP_NUM_THREADS=1;export MKL_NUM_THREADS=1;export NUMEXPR_NUM_THREADS=1;export TF_NUM_INTEROP_THREADS=1;export TF_NUM_INTRAOP_THREADS=1;export TF_CPP_MIN_LOG_LEVEL=1;export TF_FORCE_GPU_ALLOW_GROWTH=true;export PYTHONUNBUFFERED=1; # Train;python -u train_and_eval_labs.py> in cluster <chimera> Done

Job <#BSUB -P acc_bedsore_images;#BSUB -q gpu;#BSUB -gpu num=1;#BSUB -R a100;#BSUB -R "span[hosts=1]";#BSUB -n 4;#BSUB -R "rusage[mem=16000]";#BSUB -W 12:00;#BSUB -o datathon_train_labs.out;#BSUB -e datathon_train_labs.err; set -euo pipefail; # Modules;ml purge;ml cuda/11.8.0;ml anaconda3/2024.06; # Conda activate (non-interactive safe);eval "$(conda shell.bash hook)";conda activate pressureulcerstudy; # Ensure CUDA runtime is visible for TF;export LD_LIBRARY_PATH="${CUDA_HOME}/lib64:${LD_LIBRARY_PATH:-}"; # Thread/env knobs;export OMP_NUM_THREADS=1;export MKL_NUM_THREADS=1;export NUMEXPR_NUM_THREADS=1;export TF_NUM_INTEROP_THREADS=1;export TF_NUM_INTRAOP_THREADS=1;export TF_CPP_MIN_LOG_LEVEL=1;export TF_FORCE_GPU_ALLOW_GROWTH=true;export PYTHONUNBUFFERED=1; # Train;python -u train_and_eval_labs.py> was submitted from host <li04e01> by user <parikr12> in cluster <chimera> at Sat Nov  8 15:34:35 2025
Job was executed on host(s) <4*lg07c07>, in queue <gpu>, as user <parikr12> in cluster <chimera> at Sat Nov  8 15:34:38 2025
</hpc/users/parikr12> was used as the home directory.
</sc/arion/projects/bedsore_images/Datathon/codedependency> was used as the working directory.
Started at Sat Nov  8 15:34:38 2025
Terminated at Sat Nov  8 15:48:36 2025
Results reported at Sat Nov  8 15:48:36 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -P acc_bedsore_images
#BSUB -q gpu
#BSUB -gpu num=1
#BSUB -R a100
#BSUB -R "span[hosts=1]"
#BSUB -n 4
#BSUB -R "rusage[mem=16000]"
#BSUB -W 12:00
#BSUB -o datathon_train_labs.out
#BSUB -e datathon_train_labs.err

set -euo pipefail

# Modules
ml purge
ml cuda/11.8.0
ml anaconda3/2024.06

# Conda activate (non-interactive safe)
eval "$(conda shell.bash hook)"
conda activate pressureulcerstudy

# Ensure CUDA runtime is visible for TF
export LD_LIBRARY_PATH="${CUDA_HOME}/lib64:${LD_LIBRARY_PATH:-}"

# Thread/env knobs
export OMP_NUM_THREADS=1
export MKL_NUM_THREADS=1
export NUMEXPR_NUM_THREADS=1
export TF_NUM_INTEROP_THREADS=1
export TF_NUM_INTRAOP_THREADS=1
export TF_CPP_MIN_LOG_LEVEL=1
export TF_FORCE_GPU_ALLOW_GROWTH=true
export PYTHONUNBUFFERED=1

# Train
python -u train_and_eval_labs.py
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   2730.00 sec.
    Max Memory :                                 694 MB
    Average Memory :                             676.66 MB
    Total Requested Memory :                     64000.00 MB
    Delta Memory :                               63306.00 MB
    Max Swap :                                   -
    Max Processes :                              10
    Max Threads :                                23
    Run time :                                   838 sec.
    Turnaround time :                            841 sec.

The output (if any) follows:


>>>Running XGBClassifier(base_score=None, booster=None, callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device=None, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=0.03, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=1000,
              n_jobs=None, num_parallel_tree=None, ...)
Fitting 5 folds for each of 100 candidates, totalling 500 fits
Grid fit complete
Predictions made
Making results
Results for XGB:
Accuracy: 0.7609
Accuracy improvement over baseline: -9.48%
F1 Score: 0.5327
F1 improvement over baseline: 53266331658291.45%
Recall: 0.8548387096774194
Best parameters: {'model__colsample_bytree': 0.75, 'model__max_depth': 1, 'model__scale_pos_weight': 8}

>>>Running KNeighborsClassifier()
Fitting 5 folds for each of 44 candidates, totalling 220 fits
Grid fit complete
Predictions made
Making results
Results for KNN:
Accuracy: 0.7892
Accuracy improvement over baseline: -6.12%
F1 Score: 0.2115
F1 improvement over baseline: 21153846153846.16%
Recall: 0.1774193548387097
Best parameters: {'model__n_neighbors': 1, 'model__p': 1, 'model__weights': 'uniform'}

>>>Running SVC(kernel='linear', random_state=42)
Fitting 5 folds for each of 18 candidates, totalling 90 fits
Grid fit complete
Predictions made
Making results
Results for SVC Linear:
Accuracy: 0.7584
Accuracy improvement over baseline: -9.79%
F1 Score: 0.5104
F1 improvement over baseline: 51041666666666.66%
Recall: 0.7903225806451613
Best parameters: {'model__C': 0.1, 'model__class_weight': 'balanced'}

>>>Running SVC(random_state=42)
Fitting 5 folds for each of 18 candidates, totalling 90 fits
Grid fit complete
Predictions made
Making results
Results for SVC RBF:
Accuracy: 0.7763
Accuracy improvement over baseline: -7.65%
F1 Score: 0.5193
F1 improvement over baseline: 51933701657458.56%
Recall: 0.7580645161290323
Best parameters: {'model__C': 1.0, 'model__class_weight': 'balanced'}

>>>Running SVC(kernel='poly', random_state=42)
Fitting 5 folds for each of 640 candidates, totalling 3200 fits
Grid fit complete
Predictions made
Making results
Results for SVC Poly:
Accuracy: 0.7635
Accuracy improvement over baseline: -9.17%
F1 Score: 0.5258
F1 improvement over baseline: 52577319587628.87%
Recall: 0.8225806451612904
Best parameters: {'model__C': 0.1, 'model__class_weight': 'balanced', 'model__coef0': 1.0, 'model__degree': 2, 'model__gamma': 'auto'}


PS:

Read file <datathon_train_labs.err> for stderr output of this job.

Sender: LSF System <lsfadmin@lg07c06>
Subject: Job 211084955: <#BSUB -P acc_bedsore_images;#BSUB -q gpu;#BSUB -gpu num=1;#BSUB -R a100;#BSUB -R "span[hosts=1]";#BSUB -n 4;#BSUB -R "rusage[mem=16000]";#BSUB -W 12:00;#BSUB -o datathon_train_labs.out;#BSUB -e datathon_train_labs.err; set -euo pipefail; # Modules;ml purge;ml cuda/11.8.0;ml anaconda3/2024.06; # Conda activate (non-interactive safe);eval "$(conda shell.bash hook)";conda activate pressureulcerstudy; # Ensure CUDA runtime is visible for TF;export LD_LIBRARY_PATH="${CUDA_HOME}/lib64:${LD_LIBRARY_PATH:-}"; # Thread/env knobs;export OMP_NUM_THREADS=1;export MKL_NUM_THREADS=1;export NUMEXPR_NUM_THREADS=1;export TF_NUM_INTEROP_THREADS=1;export TF_NUM_INTRAOP_THREADS=1;export TF_CPP_MIN_LOG_LEVEL=1;export TF_FORCE_GPU_ALLOW_GROWTH=true;export PYTHONUNBUFFERED=1; # Train;python -u train_and_eval_labs.py> in cluster <chimera> Done

Job <#BSUB -P acc_bedsore_images;#BSUB -q gpu;#BSUB -gpu num=1;#BSUB -R a100;#BSUB -R "span[hosts=1]";#BSUB -n 4;#BSUB -R "rusage[mem=16000]";#BSUB -W 12:00;#BSUB -o datathon_train_labs.out;#BSUB -e datathon_train_labs.err; set -euo pipefail; # Modules;ml purge;ml cuda/11.8.0;ml anaconda3/2024.06; # Conda activate (non-interactive safe);eval "$(conda shell.bash hook)";conda activate pressureulcerstudy; # Ensure CUDA runtime is visible for TF;export LD_LIBRARY_PATH="${CUDA_HOME}/lib64:${LD_LIBRARY_PATH:-}"; # Thread/env knobs;export OMP_NUM_THREADS=1;export MKL_NUM_THREADS=1;export NUMEXPR_NUM_THREADS=1;export TF_NUM_INTEROP_THREADS=1;export TF_NUM_INTRAOP_THREADS=1;export TF_CPP_MIN_LOG_LEVEL=1;export TF_FORCE_GPU_ALLOW_GROWTH=true;export PYTHONUNBUFFERED=1; # Train;python -u train_and_eval_labs.py> was submitted from host <li04e04> by user <parikr12> in cluster <chimera> at Sat Nov  8 20:04:58 2025
Job was executed on host(s) <4*lg07c06>, in queue <gpu>, as user <parikr12> in cluster <chimera> at Sat Nov  8 20:05:00 2025
</hpc/users/parikr12> was used as the home directory.
</sc/arion/projects/bedsore_images/Datathon/codedependency> was used as the working directory.
Started at Sat Nov  8 20:05:00 2025
Terminated at Sat Nov  8 20:18:41 2025
Results reported at Sat Nov  8 20:18:41 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -P acc_bedsore_images
#BSUB -q gpu
#BSUB -gpu num=1
#BSUB -R a100
#BSUB -R "span[hosts=1]"
#BSUB -n 4
#BSUB -R "rusage[mem=16000]"
#BSUB -W 12:00
#BSUB -o datathon_train_labs.out
#BSUB -e datathon_train_labs.err

set -euo pipefail

# Modules
ml purge
ml cuda/11.8.0
ml anaconda3/2024.06

# Conda activate (non-interactive safe)
eval "$(conda shell.bash hook)"
conda activate pressureulcerstudy

# Ensure CUDA runtime is visible for TF
export LD_LIBRARY_PATH="${CUDA_HOME}/lib64:${LD_LIBRARY_PATH:-}"

# Thread/env knobs
export OMP_NUM_THREADS=1
export MKL_NUM_THREADS=1
export NUMEXPR_NUM_THREADS=1
export TF_NUM_INTEROP_THREADS=1
export TF_NUM_INTRAOP_THREADS=1
export TF_CPP_MIN_LOG_LEVEL=1
export TF_FORCE_GPU_ALLOW_GROWTH=true
export PYTHONUNBUFFERED=1

# Train
python -u train_and_eval_labs.py
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   2515.00 sec.
    Max Memory :                                 706 MB
    Average Memory :                             681.71 MB
    Total Requested Memory :                     64000.00 MB
    Delta Memory :                               63294.00 MB
    Max Swap :                                   -
    Max Processes :                              10
    Max Threads :                                23
    Run time :                                   821 sec.
    Turnaround time :                            823 sec.

The output (if any) follows:


>>>Running XGBClassifier(base_score=None, booster=None, callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device=None, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=0.03, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=1000,
              n_jobs=None, num_parallel_tree=None, ...)
Fitting 5 folds for each of 100 candidates, totalling 500 fits
Grid fit complete
Predictions made
Making results
Results for XGB:
Accuracy: 0.8123
Accuracy improvement over baseline: -4.82%
F1 Score: 0.4427
F1 improvement over baseline: 44274809160305.34%
Recall: 0.5087719298245614
Best parameters: {'model__colsample_bytree': 0.75, 'model__max_depth': 3, 'model__scale_pos_weight': 10}

>>>Running KNeighborsClassifier()
Fitting 5 folds for each of 44 candidates, totalling 220 fits
Grid fit complete
Predictions made
Making results
Results for KNN:
Accuracy: 0.8021
Accuracy improvement over baseline: -6.02%
F1 Score: 0.3186
F1 improvement over baseline: 31858407079646.02%
Recall: 0.3157894736842105
Best parameters: {'model__n_neighbors': 1, 'model__p': 2, 'model__weights': 'uniform'}

>>>Running SVC(kernel='linear', random_state=42)
Fitting 5 folds for each of 18 candidates, totalling 90 fits
Grid fit complete
Predictions made
Making results
Results for SVC Linear:
Accuracy: 0.7275
Accuracy improvement over baseline: -14.76%
F1 Score: 0.4536
F1 improvement over baseline: 45360824742268.04%
Recall: 0.7719298245614035
Best parameters: {'model__C': 1.0, 'model__class_weight': 'balanced'}

>>>Running SVC(random_state=42)
Fitting 5 folds for each of 18 candidates, totalling 90 fits
Grid fit complete
Predictions made
Making results
Results for SVC RBF:
Accuracy: 0.7429
Accuracy improvement over baseline: -12.95%
F1 Score: 0.4318
F1 improvement over baseline: 43181818181818.18%
Recall: 0.6666666666666666
Best parameters: {'model__C': 1.0, 'model__class_weight': 'balanced'}

>>>Running SVC(kernel='poly', random_state=42)
Fitting 5 folds for each of 640 candidates, totalling 3200 fits
Grid fit complete
Predictions made
Making results
Results for SVC Poly:
Accuracy: 0.7558
Accuracy improvement over baseline: -11.45%
F1 Score: 0.4633
F1 improvement over baseline: 46327683615819.21%
Recall: 0.7192982456140351
Best parameters: {'model__C': 0.01, 'model__class_weight': 'balanced', 'model__coef0': 1.0, 'model__degree': 3, 'model__gamma': 'scale'}


PS:

Read file <datathon_train_labs.err> for stderr output of this job.

Sender: LSF System <lsfadmin@lg07c06>
Subject: Job 211086364: <#BSUB -P acc_bedsore_images;#BSUB -q gpu;#BSUB -gpu num=1;#BSUB -R a100;#BSUB -R "span[hosts=1]";#BSUB -n 4;#BSUB -R "rusage[mem=16000]";#BSUB -W 12:00;#BSUB -o datathon_train_labs.out;#BSUB -e datathon_train_labs.err; set -euo pipefail; # Modules;ml purge;ml cuda/11.8.0;ml anaconda3/2024.06; # Conda activate (non-interactive safe);eval "$(conda shell.bash hook)";conda activate pressureulcerstudy; # Ensure CUDA runtime is visible for TF;export LD_LIBRARY_PATH="${CUDA_HOME}/lib64:${LD_LIBRARY_PATH:-}"; # Thread/env knobs;export OMP_NUM_THREADS=1;export MKL_NUM_THREADS=1;export NUMEXPR_NUM_THREADS=1;export TF_NUM_INTEROP_THREADS=1;export TF_NUM_INTRAOP_THREADS=1;export TF_CPP_MIN_LOG_LEVEL=1;export TF_FORCE_GPU_ALLOW_GROWTH=true;export PYTHONUNBUFFERED=1; # Train;python -u train_and_eval_labs.py> in cluster <chimera> Done

Job <#BSUB -P acc_bedsore_images;#BSUB -q gpu;#BSUB -gpu num=1;#BSUB -R a100;#BSUB -R "span[hosts=1]";#BSUB -n 4;#BSUB -R "rusage[mem=16000]";#BSUB -W 12:00;#BSUB -o datathon_train_labs.out;#BSUB -e datathon_train_labs.err; set -euo pipefail; # Modules;ml purge;ml cuda/11.8.0;ml anaconda3/2024.06; # Conda activate (non-interactive safe);eval "$(conda shell.bash hook)";conda activate pressureulcerstudy; # Ensure CUDA runtime is visible for TF;export LD_LIBRARY_PATH="${CUDA_HOME}/lib64:${LD_LIBRARY_PATH:-}"; # Thread/env knobs;export OMP_NUM_THREADS=1;export MKL_NUM_THREADS=1;export NUMEXPR_NUM_THREADS=1;export TF_NUM_INTEROP_THREADS=1;export TF_NUM_INTRAOP_THREADS=1;export TF_CPP_MIN_LOG_LEVEL=1;export TF_FORCE_GPU_ALLOW_GROWTH=true;export PYTHONUNBUFFERED=1; # Train;python -u train_and_eval_labs.py> was submitted from host <li04e04> by user <parikr12> in cluster <chimera> at Sat Nov  8 20:24:39 2025
Job was executed on host(s) <4*lg07c06>, in queue <gpu>, as user <parikr12> in cluster <chimera> at Sat Nov  8 20:24:40 2025
</hpc/users/parikr12> was used as the home directory.
</sc/arion/projects/bedsore_images/Datathon/codedependency> was used as the working directory.
Started at Sat Nov  8 20:24:40 2025
Terminated at Sat Nov  8 20:42:27 2025
Results reported at Sat Nov  8 20:42:27 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -P acc_bedsore_images
#BSUB -q gpu
#BSUB -gpu num=1
#BSUB -R a100
#BSUB -R "span[hosts=1]"
#BSUB -n 4
#BSUB -R "rusage[mem=16000]"
#BSUB -W 12:00
#BSUB -o datathon_train_labs.out
#BSUB -e datathon_train_labs.err

set -euo pipefail

# Modules
ml purge
ml cuda/11.8.0
ml anaconda3/2024.06

# Conda activate (non-interactive safe)
eval "$(conda shell.bash hook)"
conda activate pressureulcerstudy

# Ensure CUDA runtime is visible for TF
export LD_LIBRARY_PATH="${CUDA_HOME}/lib64:${LD_LIBRARY_PATH:-}"

# Thread/env knobs
export OMP_NUM_THREADS=1
export MKL_NUM_THREADS=1
export NUMEXPR_NUM_THREADS=1
export TF_NUM_INTEROP_THREADS=1
export TF_NUM_INTRAOP_THREADS=1
export TF_CPP_MIN_LOG_LEVEL=1
export TF_FORCE_GPU_ALLOW_GROWTH=true
export PYTHONUNBUFFERED=1

# Train
python -u train_and_eval_labs.py
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   3000.00 sec.
    Max Memory :                                 701 MB
    Average Memory :                             679.16 MB
    Total Requested Memory :                     64000.00 MB
    Delta Memory :                               63299.00 MB
    Max Swap :                                   -
    Max Processes :                              10
    Max Threads :                                23
    Run time :                                   1067 sec.
    Turnaround time :                            1068 sec.

The output (if any) follows:


>>>Running XGBClassifier(base_score=None, booster=None, callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device=None, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=0.03, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=1000,
              n_jobs=None, num_parallel_tree=None, ...)
Fitting 5 folds for each of 100 candidates, totalling 500 fits
Grid fit complete
Predictions made
Making results
Results for XGB:
Accuracy: 0.7635
Accuracy improvement over baseline: -9.17%
F1 Score: 0.5354
F1 improvement over baseline: 53535353535353.54%
Recall: 0.8548387096774194
Best parameters: {'model__colsample_bytree': 0.25, 'model__max_depth': 1, 'model__scale_pos_weight': 8}

>>>Running KNeighborsClassifier()
Fitting 5 folds for each of 44 candidates, totalling 220 fits
Grid fit complete
Predictions made
Making results
Results for KNN:
Accuracy: 0.7995
Accuracy improvement over baseline: -4.89%
F1 Score: 0.2642
F1 improvement over baseline: 26415094339622.64%
Recall: 0.22580645161290322
Best parameters: {'model__n_neighbors': 1, 'model__p': 2, 'model__weights': 'uniform'}

>>>Running SVC(kernel='linear', random_state=42)
Fitting 5 folds for each of 18 candidates, totalling 90 fits
Grid fit complete
Predictions made
Making results
Results for SVC Linear:
Accuracy: 0.7892
Accuracy improvement over baseline: -6.12%
F1 Score: 0.5393
F1 improvement over baseline: 53932584269662.92%
Recall: 0.7741935483870968
Best parameters: {'model__C': 0.001, 'model__class_weight': 'balanced'}

>>>Running SVC(random_state=42)
Fitting 5 folds for each of 18 candidates, totalling 90 fits
Grid fit complete
Predictions made
Making results
Results for SVC RBF:
Accuracy: 0.7044
Accuracy improvement over baseline: -16.21%
F1 Score: 0.5022
F1 improvement over baseline: 50216450216450.21%
Recall: 0.9354838709677419
Best parameters: {'model__C': 0.1, 'model__class_weight': 'balanced'}

>>>Running SVC(kernel='poly', random_state=42)
Fitting 5 folds for each of 640 candidates, totalling 3200 fits
Grid fit complete
Predictions made
Making results
Results for SVC Poly:
Accuracy: 0.7609
Accuracy improvement over baseline: -9.48%
F1 Score: 0.5231
F1 improvement over baseline: 52307692307692.31%
Recall: 0.8225806451612904
Best parameters: {'model__C': 0.1, 'model__class_weight': 'balanced', 'model__coef0': 1.0, 'model__degree': 2, 'model__gamma': 'auto'}


PS:

Read file <datathon_train_labs.err> for stderr output of this job.

